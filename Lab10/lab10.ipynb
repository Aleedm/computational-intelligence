{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Alessandro De Marco`<a.demarco@studenti.polito.it>`  \n",
    "[Github](https://github.com/Aleedm)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/Aleedm/computational-intelligence/blob/3e7d43ccabdc267b53f693df02c6b39eecc2f21f/LICENSE) for details.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "from collections import namedtuple, defaultdict\n",
    "from random import choice, random, choices\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "\n",
    "State = namedtuple('State', ['x', 'o', 'turn'])\n",
    "MAGIC = [2, 7, 6, \n",
    "         9, 5, 1, \n",
    "         4, 3, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 10_000_000\n",
    "selection_factor = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Usefull functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_board(pos):\n",
    "    \"\"\"Nicely prints the board\"\"\"\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c\n",
    "            if MAGIC[i] in pos.x:\n",
    "                print('X', end='')\n",
    "            elif MAGIC[i] in pos.o:\n",
    "                print('O', end='')\n",
    "            else:\n",
    "                print('.', end='')\n",
    "        print()\n",
    "    print()\n",
    "    \n",
    "def win(elements):\n",
    "    \"\"\"Checks is elements is winning\"\"\"\n",
    "    return any(sum(c) == 15 for c in combinations(elements, 3))\n",
    "\n",
    "def state_value(pos: State):\n",
    "    \"\"\"Evaluate state: +1 first player wins\"\"\"\n",
    "    if win(pos.x):\n",
    "        return 1\n",
    "    elif win(pos.o):\n",
    "        return -1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_weights(weights):\n",
    "    total = sum(weights)\n",
    "    if total == 0:\n",
    "        return [1/len(weights) for _ in weights]  \n",
    "    return [w / total for w in weights]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_state_action_key(game_state, action, next_turn=None):\n",
    "    if next_turn is None:\n",
    "        next_turn = game_state.turn\n",
    "    if next_turn == 'o':\n",
    "        x_positions = frozenset(game_state.x).union([action])\n",
    "        o_positions = frozenset(game_state.o)\n",
    "    else:\n",
    "        x_positions = frozenset(game_state.x)\n",
    "        o_positions = frozenset(game_state.o).union([action])\n",
    "    return (x_positions, o_positions, action, next_turn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_best_action_q_learning(game_state, q_values, select_best=True):\n",
    "    available_actions = [a for a in MAGIC if a not in game_state.x and a not in game_state.o]\n",
    "    if not available_actions:\n",
    "        return None\n",
    "    action_values = []\n",
    "    next_turn = 'o' if game_state.turn == 'x' else 'x'\n",
    "\n",
    "    for action in available_actions:\n",
    "        key = create_state_action_key(game_state, action, next_turn)\n",
    "        value = q_values[key]\n",
    "        action_values.append((action, value))\n",
    "\n",
    "    action_values.sort(key=lambda x: x[1], reverse=(game_state.turn == 'x'))\n",
    "    \n",
    "    n_best_actions = len(available_actions) // selection_factor + (len(available_actions) % selection_factor > 0)\n",
    "    best_actions = action_values[:n_best_actions]\n",
    "    \n",
    "    if select_best:\n",
    "        return best_actions[0][0]\n",
    "\n",
    "    if game_state.turn == 'x':\n",
    "        weights = [value for _, value in best_actions]\n",
    "    else:\n",
    "        max_value = max(value for _, value in best_actions)\n",
    "        weights = [max_value - value + 0.1 for _, value in best_actions]\n",
    "        \n",
    "    normalized_weights = normalize_weights(weights)\n",
    "    \n",
    "    selected_action = choices([action for action, _ in best_actions], weights=normalized_weights, k=1)[0]\n",
    "\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_random_game(q_values, epsilon=1):\n",
    "    episode = []\n",
    "    player_x_positions = set()\n",
    "    player_o_positions = set()\n",
    "    current_turn = 'x'\n",
    "    available_actions = set(MAGIC)\n",
    "\n",
    "    while available_actions:\n",
    "        if random() < epsilon:\n",
    "            action = choice(list(available_actions))\n",
    "        else:\n",
    "            game_state = State(player_x_positions, player_o_positions, current_turn)\n",
    "            action = select_best_action_q_learning(game_state, q_values)\n",
    "\n",
    "        if current_turn == 'x':\n",
    "            player_x_positions.add(action)\n",
    "            current_turn = 'o'\n",
    "        else:\n",
    "            player_o_positions.add(action)\n",
    "            current_turn = 'x'\n",
    "        episode_state = State(copy.copy(player_x_positions), copy.copy(player_o_positions), current_turn)\n",
    "        episode.append((episode_state, action))\n",
    "        available_actions.remove(action)\n",
    "\n",
    "        if win(player_x_positions) or win(player_o_positions) or not available_actions:\n",
    "            break\n",
    "\n",
    "    return episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.1\n",
    "discount_factor = 0.9\n",
    "\n",
    "def train_q_learning(total_episodes, learning_rate, discount_factor):\n",
    "    q_values = defaultdict(float)\n",
    "    \n",
    "    epsilon = 1\n",
    "    epsilon_min = 0.3\n",
    "    epsilon_decay = (epsilon - epsilon_min) / (total_episodes // 2)\n",
    "    \n",
    "    for episode_number in tqdm(range(total_episodes)):\n",
    "        if episode_number >= total_episodes // 2:\n",
    "            epsilon = max(epsilon - epsilon_decay, epsilon_min)\n",
    "        episode = generate_random_game(q_values, epsilon)\n",
    "        final_reward = state_value(episode[-1][0])\n",
    "        for game_state, action in reversed(episode):\n",
    "            key = create_state_action_key(game_state, action)\n",
    "            future_rewards = [q_values[create_state_action_key(game_state, a)] for a in MAGIC if a not in game_state.x and a not in game_state.o]\n",
    "            max_future_reward = max(future_rewards, default=0)\n",
    "            q_values[key] = q_values[key] + learning_rate * (final_reward + discount_factor * max_future_reward - q_values[key])\n",
    "            final_reward = q_values[key] \n",
    "    return q_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_values = train_q_learning(episodes, learning_rate, discount_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_action_monte_carlo(game_state, state_values):\n",
    "    available_actions = [a for a in MAGIC if a not in game_state.x and a not in game_state.o]\n",
    "    if not available_actions:\n",
    "        return None\n",
    "    action_value_pairs = []\n",
    "\n",
    "    for action in available_actions:\n",
    "        next_state = State(\n",
    "            frozenset(game_state.x.union([action])) if game_state.turn == 'x' else game_state.x, \n",
    "            frozenset(game_state.o.union([action])) if game_state.turn == 'o' else game_state.o, \n",
    "            'o' if game_state.turn == 'x' else 'x'\n",
    "        )\n",
    "        value = state_values.get(next_state, 0)\n",
    "        action_value_pairs.append((action, value))\n",
    "\n",
    "    action_value_pairs.sort(key=lambda x: x[1], reverse=(game_state.turn == 'x'))\n",
    "    selection_factor = 3\n",
    "    n_best_actions = len(available_actions) // selection_factor + (len(available_actions) % selection_factor > 0)\n",
    "    best_actions = action_value_pairs[:n_best_actions]\n",
    "\n",
    "    if game_state.turn == 'x':\n",
    "        weights = [value for _, value in best_actions]\n",
    "    else:\n",
    "        max_value = max(value for _, value in best_actions)\n",
    "        weights = [max_value - value + 0.1 for _, value in best_actions]\n",
    "\n",
    "    normalized_weights = normalize_weights(weights)\n",
    "    selected_action = choices([action for action, _ in best_actions], weights=normalized_weights, k=1)[0]\n",
    "\n",
    "    return selected_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_monte_carlo_episode():\n",
    "    episode = []\n",
    "    game_state = State(frozenset(), frozenset(), 'x')\n",
    "    available_actions = set(MAGIC)\n",
    "\n",
    "    while True:\n",
    "        action = choice(list(available_actions))\n",
    "        available_actions.remove(action)\n",
    "        \n",
    "        if game_state.turn == 'x':\n",
    "            new_x_positions = game_state.x.union([action])\n",
    "            game_state = State(frozenset(new_x_positions), game_state.o, 'o')\n",
    "        else:\n",
    "            new_o_positions = game_state.o.union([action])\n",
    "            game_state = State(game_state.x, frozenset(new_o_positions), 'x')\n",
    "\n",
    "        episode.append(game_state)\n",
    "\n",
    "        if win(game_state.x) or win(game_state.o) or len(game_state.x) + len(game_state.o) == 9:\n",
    "            break\n",
    "\n",
    "    reward = 1 if win(game_state.x) else -1 if win(game_state.o) else 0\n",
    "    return [(s, reward) for s in episode]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_monte_carlo(total_episodes):\n",
    "    state_values = defaultdict(float)\n",
    "    state_counts = defaultdict(int)\n",
    "\n",
    "    for _ in tqdm(range(total_episodes)):\n",
    "        episode = generate_monte_carlo_episode()\n",
    "        total_return = 0\n",
    "        for game_state, reward in reversed(episode):\n",
    "            total_return = reward + total_return\n",
    "            state_counts[game_state] += 1\n",
    "            state_values[game_state] += (total_return - state_values[game_state]) / state_counts[game_state]\n",
    "\n",
    "    return state_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_values_mc = train_monte_carlo(episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_agent(state, _):\n",
    "    available_moves = [a for a in MAGIC if a not in state.x and a not in state.o]\n",
    "    return choice(available_moves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_state(state, move):\n",
    "    new_x = state.x.union([move]) if state.turn == 'x' else state.x\n",
    "    new_o = state.o.union([move]) if state.turn == 'o' else state.o\n",
    "    new_turn = 'o' if state.turn == 'x' else 'x'\n",
    "    return State(new_x, new_o, new_turn)\n",
    "\n",
    "def is_terminal(state):\n",
    "    return win(state.x) or win(state.o) or len(state.x) + len(state.o) == 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_results(state, results, last_player, agent1_starts, agent1_name, agent2_name, initial_moves, turn_count):\n",
    "    if win(state.x):\n",
    "        winner = agent1_name if agent1_starts else agent2_name\n",
    "        loser = agent2_name if agent1_starts else agent1_name\n",
    "        result = \"Win\"\n",
    "    elif win(state.o):\n",
    "        winner = agent2_name if agent1_starts else agent1_name\n",
    "        loser = agent1_name if agent1_starts else agent2_name\n",
    "        result = \"Loss\"\n",
    "    else:\n",
    "        winner = loser = None\n",
    "        result = \"Draw\"\n",
    "\n",
    "    game_result_info_w = {\n",
    "        \"Initial Moves\": initial_moves,\n",
    "        \"Turn Count\": turn_count,\n",
    "        \"Result\": \"Win\"\n",
    "    }\n",
    "    game_result_info_l = {\n",
    "        \"Initial Moves\": initial_moves,\n",
    "        \"Turn Count\": turn_count,\n",
    "        \"Result\": \"Loss\"\n",
    "    }\n",
    "\n",
    "    if winner:\n",
    "        if agent1_name == winner and agent1_starts:\n",
    "            results[agent1_name][\"Starts First\"][\"Games\"].append(game_result_info_w)\n",
    "            results[agent2_name][\"Starts Second\"][\"Games\"].append(game_result_info_l)\n",
    "        elif agent1_name == winner and not agent1_starts:\n",
    "            results[agent1_name][\"Starts Second\"][\"Games\"].append(game_result_info_w)\n",
    "            results[agent2_name][\"Starts First\"][\"Games\"].append(game_result_info_l)\n",
    "        elif agent2_name == winner and agent1_starts:\n",
    "            results[agent2_name][\"Starts Second\"][\"Games\"].append(game_result_info_w)\n",
    "            results[agent1_name][\"Starts First\"][\"Games\"].append(game_result_info_l)\n",
    "        elif agent2_name == winner and not agent1_starts:\n",
    "            results[agent2_name][\"Starts First\"][\"Games\"].append(game_result_info_w)\n",
    "            results[agent1_name][\"Starts Second\"][\"Games\"].append(game_result_info_l)\n",
    "    else:\n",
    "        game_result_info = {\n",
    "            \"Initial Moves\": initial_moves,\n",
    "            \"Turn Count\": turn_count,\n",
    "            \"Result\": \"Draw\"\n",
    "        }\n",
    "        results[agent1_name][\"Starts First\" if agent1_starts else \"Starts Second\"][\"Games\"].append(game_result_info)\n",
    "        results[agent2_name][\"Starts Second\" if agent1_starts else \"Starts First\"][\"Games\"].append(game_result_info)\n",
    "\n",
    "\n",
    "def compete(num_games, agent1_name, choose_move_agent1, agent1_dict, agent2_name, choose_move_agent2, agent2_dict):\n",
    "    results = {\n",
    "        agent1_name: {\n",
    "            \"Starts First\": {\"Games\": []},\n",
    "            \"Starts Second\": {\"Games\": []}\n",
    "        },\n",
    "        agent2_name: {\n",
    "            \"Starts First\": {\"Games\": []},\n",
    "            \"Starts Second\": {\"Games\": []}\n",
    "        }\n",
    "    }\n",
    "    agent1_starts = True \n",
    "\n",
    "    for _ in range(num_games):\n",
    "        state = State(frozenset(), frozenset(), 'x')\n",
    "        is_agent1_turn = agent1_starts\n",
    "        turn_count = 0\n",
    "        initial_moves = []\n",
    "\n",
    "        while True:\n",
    "            if is_agent1_turn:\n",
    "                move = choose_move_agent1(state, agent1_dict)\n",
    "                player = agent1_name\n",
    "            else:\n",
    "                move = choose_move_agent2(state, agent2_dict)\n",
    "                player = agent2_name\n",
    "\n",
    "            if turn_count < 2:\n",
    "                initial_moves.append(move)\n",
    "\n",
    "            state = update_state(state, move)\n",
    "            turn_count += 1\n",
    "\n",
    "            if is_terminal(state):\n",
    "                update_results(state, results, player, agent1_starts, agent1_name, agent2_name, initial_moves, turn_count)\n",
    "                break\n",
    "\n",
    "            is_agent1_turn = not is_agent1_turn\n",
    "        agent1_starts = not agent1_starts\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_games = 100_000\n",
    "results_mc_ql = compete(num_games, \"Montecarlo\", select_best_action_monte_carlo, state_values_mc, \"Q-Learning\", select_best_action_q_learning, q_values)\n",
    "results_mc_rn = compete(num_games, \"Montecarlo\", select_best_action_monte_carlo, state_values_mc, \"Random\", random_agent, None)\n",
    "results_ql_rn = compete(num_games, \"Q-Learning\", select_best_action_q_learning, q_values, \"Random\", random_agent, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot montecarlo vs q-learning\n",
    "plot_total_outcomes(results_mc_ql, \"total_outcomes_mc_ql.png\")\n",
    "plot_outcomes_by_starting_agent(results_mc_ql, \"outcomes_by_starting_agent_mc_ql.png\")\n",
    "plot_wins_draws_by_turns(results_mc_ql, \"wins_draws_by_turns_mc_ql.png\")\n",
    "plot_top_two_moves_podium(results_mc_ql, \"top_two_moves_podium_mc_ql.png\")\n",
    "#Plot montecarlo vs random\n",
    "plot_total_outcomes(results_mc_rn, \"total_outcomes_mc_rn.png\")\n",
    "plot_outcomes_by_starting_agent(results_mc_rn, \"outcomes_by_starting_agent_mc_rn.png\")\n",
    "plot_wins_draws_by_turns(results_mc_rn, \"wins_draws_by_turns_mc_rn.png\")\n",
    "plot_top_two_moves_podium(results_mc_rn, \"top_two_moves_podium_mc_rn.png\")\n",
    "#Plot q-learning vs random\n",
    "plot_total_outcomes(results_ql_rn, \"total_outcomes_ql_rn.png\")\n",
    "plot_outcomes_by_starting_agent(results_ql_rn, \"outcomes_by_starting_agent_ql_rn.png\")\n",
    "plot_wins_draws_by_turns(results_ql_rn, \"wins_draws_by_turns_ql_rn.png\")\n",
    "plot_top_two_moves_podium(results_ql_rn, \"top_two_moves_podium_ql_rn.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_total_outcomes(results, filename=\"total_outcomes.png\"):\n",
    "    agents_name = list(results.keys())\n",
    "    total_wins = {agent: 0 for agent in results}\n",
    "    total_draws = 0\n",
    "\n",
    "    for agent, data in results.items():\n",
    "        for position in [\"Starts First\", \"Starts Second\"]:\n",
    "            for game in data[position][\"Games\"]:\n",
    "                if game[\"Result\"] == \"Win\":\n",
    "                    total_wins[agent] += 1\n",
    "                elif game[\"Result\"] == \"Draw\":\n",
    "                    total_draws += 1\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    outcomes = [f'{agents_name[0]} Wins', f'{agents_name[1]} Wins', 'Draws']\n",
    "    values = [total_wins[agents_name[0]], total_wins[agents_name[1]], total_draws/2]\n",
    "\n",
    "    ax.bar(outcomes, values)\n",
    "\n",
    "    ax.set_ylabel('Number of Games')\n",
    "    ax.set_title('Total Game Outcomes')\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outcomes_by_starting_agent(results, filename=\"outcomes_by_starting_agent.png\"):\n",
    "    agents_name = list(results.keys())\n",
    "    outcomes_first = {\"Win\": 0, \"Loss\": 0, \"Draw\": 0}\n",
    "    outcomes_second = {\"Win\": 0, \"Loss\": 0, \"Draw\": 0}\n",
    "\n",
    "    for game in results[agents_name[0]][\"Starts First\"][\"Games\"]:\n",
    "        outcomes_first[game[\"Result\"]] += 1\n",
    "\n",
    "    for game in results[agents_name[1]][\"Starts First\"][\"Games\"]:\n",
    "        outcomes_second[game[\"Result\"]] += 1\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n",
    "\n",
    "    ax1.bar(outcomes_first.keys(), outcomes_first.values())\n",
    "    ax1.set_title(f\"{agents_name[0]} Starts First\")\n",
    "    ax1.set_ylabel(\"Number of Games\")\n",
    "    \n",
    "    ax2.bar(outcomes_second.keys(), outcomes_second.values())\n",
    "    ax2.set_title(f\"{agents_name[1]} Starts First\")\n",
    "\n",
    "    plt.savefig(filename)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_wins_draws_by_turns(results, filename=\"wins_draws_by_turns.png\"):\n",
    "    agents_name = list(results.keys())\n",
    "    turn_counts = list(range(5, 10))\n",
    "    outcomes = [\"Win\", \"Draw\", \"Loss\"]\n",
    "    data = {agent: {outcome: [0 for _ in turn_counts] for outcome in outcomes} for agent in results}\n",
    "\n",
    "    for agent, positions in results.items():\n",
    "        for position, games in positions.items():\n",
    "            for game in games[\"Games\"]:\n",
    "                if game[\"Turn Count\"] in turn_counts:\n",
    "                    data[agent][game[\"Result\"]][game[\"Turn Count\"] - 5] += 1\n",
    "\n",
    "    n_groups = len(turn_counts)\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    index = np.arange(n_groups)\n",
    "    bar_width = 0.35\n",
    "    opacity = 0.8\n",
    "\n",
    "    rects1 = ax.bar(index, data[agents_name[0]][\"Win\"], bar_width,\n",
    "                    alpha=opacity, label=f'{agents_name[0]} Wins')\n",
    "\n",
    "    rects2 = ax.bar(index, data[agents_name[0]][\"Draw\"], bar_width, bottom=data[agents_name[0]][\"Win\"],\n",
    "                    alpha=opacity, label='Draws')\n",
    "\n",
    "    rects3 = ax.bar(index + bar_width, data[agents_name[1]][\"Win\"], bar_width,\n",
    "                    alpha=opacity, label=f'{agents_name[1]} Wins')\n",
    "\n",
    "    ax.set_xlabel('Turn Count')\n",
    "    ax.set_ylabel('Scores')\n",
    "    ax.set_title('Scores by turn count and outcome')\n",
    "    ax.set_xticks(index + bar_width / 2)\n",
    "    ax.set_xticklabels(turn_counts)\n",
    "    ax.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_two_moves_podium(results, filename_prefix=\"top_two_moves_podium\"):\n",
    "    top_moves = {agent: Counter() for agent in results}\n",
    "    top_moves_draws = Counter()\n",
    "\n",
    "    for agent, data in results.items():\n",
    "        for position in [\"Starts First\", \"Starts Second\"]:\n",
    "            for game in data[position][\"Games\"]:\n",
    "                if len(game[\"Initial Moves\"]) >= 2:\n",
    "                    move_combination = tuple(game[\"Initial Moves\"][:2])\n",
    "                    if game[\"Result\"] == \"Win\":\n",
    "                        top_moves[agent][move_combination] += 1\n",
    "                    elif game[\"Result\"] == \"Draw\":\n",
    "                        top_moves_draws[move_combination] += 1\n",
    "\n",
    "    podium_data = {agent: top_moves[agent].most_common(3) for agent in results}\n",
    "    podium_data[\"Draws\"] = top_moves_draws.most_common(3)\n",
    "\n",
    "    def draw_board(ax, moves, title):\n",
    "        ax.plot([1, 1], [0, 3], 'k', linewidth=2)\n",
    "        ax.plot([2, 2], [0, 3], 'k', linewidth=2)\n",
    "        ax.plot([0, 3], [1, 1], 'k', linewidth=2)\n",
    "        ax.plot([0, 3], [2, 2], 'k', linewidth=2)\n",
    "        for i, move in enumerate(moves):\n",
    "            symbol = 'X' if i % 2 == 0 else 'O'\n",
    "            row, col = divmod(move - 1, 3)\n",
    "            ax.text(col + 0.5, 2 - row + 0.5, symbol, fontsize=36, ha='center', va='center')\n",
    "        ax.set_title(title)\n",
    "        ax.axis('off')\n",
    "\n",
    "    for agent, top_moves in podium_data.items():\n",
    "        fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n",
    "        fig.suptitle(f'Top Two Moves for {agent}', fontsize=16)\n",
    "\n",
    "        for i, (moves, count) in enumerate(top_moves):\n",
    "            draw_board(axs[i], moves, f'Rank {i+1}: {count} wins')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"{filename_prefix}_{agent}.png\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play with the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython.display import clear_output\n",
    "import time\n",
    "\n",
    "def clear_console():\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "    clear_output(wait=True)\n",
    "    time.sleep(0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_against_agent(choose_move_agent, agent_dict):\n",
    "    state = State(frozenset(), frozenset(), 'x')\n",
    "\n",
    "    while not is_terminal(state):\n",
    "        if state.turn == 'x':\n",
    "            print_board(state)\n",
    "            print(f\"\\n{MAGIC[0:3]}\\n{MAGIC[3:6]}\\n{MAGIC[6:9]}\")\n",
    "            move = int(input(f\"Scegli la tua mossa (1-9):\"))\n",
    "            while move not in MAGIC or move in state.x or move in state.o:\n",
    "                print(\"Mossa non valida. Riprova.\")\n",
    "                move = int(input(\"Scegli la tua mossa (1-9): \"))\n",
    "            state = State(state.x.union([move]), state.o, 'o')\n",
    "        else:\n",
    "            move = choose_move_agent(state, agent_dict)\n",
    "            state = State(state.x, state.o.union([move]), 'x')\n",
    "        clear_console()\n",
    "\n",
    "    print_board(state)\n",
    "    if win(state.x):\n",
    "        print(\"Hai vinto!\")\n",
    "    elif win(state.o):\n",
    "        print(\"L'agente ha vinto.\")\n",
    "    else:\n",
    "        print(\"Pareggio.\")\n",
    "\n",
    "\n",
    "def print_board_with_choices(state):\n",
    "    for r in range(3):\n",
    "        for c in range(3):\n",
    "            i = r * 3 + c + 1\n",
    "            if i in state.x:\n",
    "                print('X', end=' ')\n",
    "            elif i in state.o:\n",
    "                print('O', end=' ')\n",
    "            else:\n",
    "                print(MAGIC[i], end=' ')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_against_agent(select_best_action_monte_carlo, state_values_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compete_interactive(num_games, q_function, state_values):\n",
    "    monte_carlo_starts = True  \n",
    "\n",
    "    for game in range(num_games):\n",
    "        print(f\"Partita {game + 1}\")\n",
    "        state = State(frozenset(), frozenset(), 'x')\n",
    "        turn = monte_carlo_starts\n",
    "        while True:\n",
    "            if turn:\n",
    "                player = \"Monte Carlo\"\n",
    "                move = select_best_action_monte_carlo(state, state_values)\n",
    "            else:\n",
    "                player = \"Q-learning\"\n",
    "                move = select_best_action_q_learning(state, q_function)\n",
    "\n",
    "            state = update_state(state, move)\n",
    "            print(f\"Mossa di {player}: {move}\")\n",
    "            print_board(state)\n",
    "\n",
    "            if is_terminal(state):\n",
    "                winner = state_value(state)\n",
    "                print(f\"Vince {player}!\" if winner == 1 else \"Pareggio!\" if winner == 0 else f\"Vince {player}!\")\n",
    "                break\n",
    "\n",
    "            input(\"Premi INVIO per continuare...\") \n",
    "            turn = not turn\n",
    "        monte_carlo_starts = not monte_carlo_starts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compete_interactive(2, q_values, state_values_mc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
